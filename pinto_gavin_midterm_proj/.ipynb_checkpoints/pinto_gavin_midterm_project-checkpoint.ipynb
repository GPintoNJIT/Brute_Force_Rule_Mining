{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d3ad19",
   "metadata": {},
   "source": [
    "**Part 1:**\n",
    "The datasets were manually created, and the transactions were created using transaction_generator.py, and stored in database. Below are necessary imports and helper functions to extract information from a file in the form of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cad69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlxtend pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import time\n",
    "\n",
    "def get_transactions_as_array (filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    transactions = df['Transaction'].tolist()\n",
    "    return transactions\n",
    "\n",
    "def get_dataset_as_array (filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    elements = df['Element'].tolist()\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7214ecc",
   "metadata": {},
   "source": [
    "**Part 2:** Implement the brute force method to generate the frequent itemsets and their association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22be1e3",
   "metadata": {},
   "source": [
    "Below are helper functions to aid with the brute force algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of occurrences of itemset in transactions\n",
    "def get_frequency (itemset, transactions):\n",
    "    frequency = 0\n",
    "    for transaction in transactions:\n",
    "        transaction_items = transaction.split(\",\")\n",
    "        if set(itemset) <= set(transaction_items):\n",
    "            frequency += 1\n",
    "    return frequency\n",
    "\n",
    "# Returns confidence of rule according to transactions\n",
    "def get_confidence (rule, transactions):\n",
    "    # formula: freq(combined) / freq(left)\n",
    "    combined = rule[0] + rule[1]\n",
    "    left = rule[0]\n",
    "    return (1.0 * get_frequency(combined, transactions)) / (1.0 * get_frequency(left, transactions))\n",
    "    \n",
    "# Returns all itemsets of size n from dataset in the form \n",
    "# list[list[], list[]] with list[0] as the itemsets and list[1] initialized to 0\n",
    "def get_k_itemsets (dataset, k):\n",
    "    k_itemsets = []\n",
    "    frequencies = []\n",
    "    for i in range(len(dataset) - k + 1):\n",
    "        element_itemset = [dataset[i]]\n",
    "        if (k > 1):\n",
    "            for second in range(i + 1, len(dataset) - k + 2):\n",
    "                element_itemset = [dataset[i]]\n",
    "                for j in range(second, second + k - 1):\n",
    "                    element_itemset.append(dataset[j])\n",
    "                k_itemsets.append(element_itemset)\n",
    "                frequencies.append(0)\n",
    "        else:\n",
    "            k_itemsets.append(element_itemset)\n",
    "            frequencies.append(0)\n",
    "    return [k_itemsets, frequencies] \n",
    "\n",
    "# Returns all possible rules from itemset in the form [[[],[]], ... ]\n",
    "def generate_rules(itemset):\n",
    "    n = len(itemset)\n",
    "    rules = []\n",
    "    for r in range(1, n):\n",
    "        for left in combinations(itemset, r):\n",
    "            left = list(left)\n",
    "            right = [item for item in itemset if item not in left]\n",
    "            rules.append([left, right])\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a05325",
   "metadata": {},
   "source": [
    "Below are the functions that generate the frequent itemsets and their association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf27199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all frequent itemsets from transactions according to min_support\n",
    "def brute_force_frequent_itemsets (dataset, transactions, min_support):\n",
    "    # Find all frequent k-itemsets\n",
    "    frequent_itemsets = []\n",
    "    frequent_itemsets_frequencies = []\n",
    "    k = 1\n",
    "    while True:\n",
    "        # Generate k-itemsets\n",
    "        k_itemset_info = get_k_itemsets(dataset, k)\n",
    "        k_itemsets = k_itemset_info[0]\n",
    "        k_itemsets_frequencies = k_itemset_info[1]\n",
    "        frequent_itemsets_found = 0\n",
    "        # Update frequencies of each itemset in transactions\n",
    "        for i in range(len(k_itemsets)):\n",
    "            k_itemsets_frequencies[i] = get_frequency(k_itemsets[i], transactions)\n",
    "        # Add frequent itemsets\n",
    "        for i in range(len(k_itemsets)):\n",
    "            if k_itemsets_frequencies[i] >= min_support:\n",
    "                frequent_itemsets.append(k_itemsets[i])\n",
    "                frequent_itemsets_frequencies.append(k_itemsets_frequencies[i])\n",
    "                frequent_itemsets_found += 1\n",
    "        # Terminate algorithm if no frequent k-itemsets found\n",
    "        if frequent_itemsets_found == 0:\n",
    "            break\n",
    "        k += 1\n",
    "    return [frequent_itemsets, frequent_itemsets_frequencies]  \n",
    "\n",
    "# Returns association rules based on min_confidence\n",
    "def brute_force_association_rules (dataset, transactions, min_support, min_confidence):\n",
    "    association_rules = []\n",
    "    # Get frequent itemsets\n",
    "    frequent_itemsets = brute_force_frequent_itemsets(dataset, transactions, min_support)[0]\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            # Get all possible rules of itemset\n",
    "            rules = generate_rules(itemset)\n",
    "            # Append all rules above min_confidence to result\n",
    "            for rule in rules:\n",
    "                conf = get_confidence(rule, transactions)\n",
    "                if conf >= min_confidence:\n",
    "                    association_rules.append([rule, conf])\n",
    "    return association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2224303",
   "metadata": {},
   "source": [
    "**Part 3:**\n",
    "Use an existing Apriori implementation from Python libraries/packages to\n",
    "verify the results from your brute force algorithm implementation.\n",
    "Use Python existing package for fpgrowth (as known as fp-tree algorithm)\n",
    "to generate the items and rules.\n",
    "Compare the results from your brute-force, Apriori, and FP-Tree/Growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686791c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user for their company choice\n",
    "print(\"Welcome! Please select the company you'd like to analyze.\")\n",
    "print(\"1. Barnes & Noble\")\n",
    "print(\"2. Costco\")\n",
    "print(\"3. Gorilla Mind\")\n",
    "print(\"4. Rogue Fitness\")\n",
    "print(\"5. Vilros\")\n",
    "user_choice = int(input(\"Please enter the number next to your choice: \"))\n",
    "\n",
    "# Prompt the user for support and confidence\n",
    "support = float(input(\"Please enter the minimum support (as a percentage): \"))\n",
    "confidence = float(input(\"Please enter the minimum confidence (as a percentage): \"))\n",
    "\n",
    "# Process which files to analyze\n",
    "companies = [\"barnes_and_noble\", \"costco\", \"gorilla_mind\", \"rogue_fitness\", \"vilros\"]\n",
    "company = companies[user_choice - 1]\n",
    "transaction_file = \"database/transaction_\" + company + \".csv\"\n",
    "dataset_file = \"database/dataset_\" + company + \".csv\"\n",
    "\n",
    "# Process support and confidence for 20 transactions\n",
    "support = (support / 100.0) * 20.0\n",
    "confidence = confidence / 100.0\n",
    "\n",
    "dataset = get_dataset_as_array(dataset_file)\n",
    "transactions = get_transactions_as_array(transaction_file)\n",
    "\n",
    "# Preprocess data for library implementations\n",
    "transactions_proper = []\n",
    "for transaction in transactions:\n",
    "    transactions_proper.append(transaction.split(\",\"))\n",
    "df = pd.DataFrame(pd.Series(transactions_proper).apply(lambda x: pd.Series(1, index=x)).fillna(0))\n",
    "\n",
    "# Start timing for Apriori\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the existing Apriori implementation\n",
    "frequent_itemsets_apriori = apriori(df, min_support=support, use_colnames=True)\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=confidence)\n",
    "\n",
    "# End timing for Apriori\n",
    "end_time = time.time()\n",
    "\n",
    "# Print Apriori results\n",
    "print(\"Frequent Itemsets (Apriori Algorithm):\")\n",
    "print(frequent_itemsets_apriori)\n",
    "print(\"\\nAssociation Rules (Apriori Algorithm):\")\n",
    "print(rules_apriori)\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTime taken for Apriori Algorithm: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Start timing for FP-Growth\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the existing package for FP-Growth\n",
    "frequent_itemsets_fp = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "rules_fp = association_rules(frequent_itemsets_fp, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# End timing for FP-Growth\n",
    "end_time = time.time()\n",
    "\n",
    "# Print FP-Growth results\n",
    "print(\"Frequent Itemsets (FP-Growth):\")\n",
    "print(frequent_itemsets_fp)\n",
    "print(\"\\nAssociation Rules (FP-Growth):\")\n",
    "print(rules_fp)\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTime taken for FP-Growth: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Start timing for brute\n",
    "start_time = time.time()\n",
    "\n",
    "# Compare the results with the brute force algorithm\n",
    "frequent_itemsets_brute = brute_force_frequent_itemsets(dataset, transactions, min_support)[0]\n",
    "rules_brute = brute_force_association_rules(dataset, transactions, support, confidence)\n",
    "\n",
    "# End timing for brute\n",
    "end_time = time.time()\n",
    "\n",
    "# Print Brute force results\n",
    "print(\"Frequent Itemsets (Brute Force):\")\n",
    "for item in frequent_itemsets_brute:\n",
    "    print(item)\n",
    "print(\"\\nAssociation Rules (Brute Force):\")\n",
    "for item in rules_brute:\n",
    "    rule = item[0]\n",
    "    conf = item[1]\n",
    "    print(\"{\", rule[0], \"} ->\", \"{\", rule[1], \"}\", \"confidence:\", conf)\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTime taken for FP-Growth: {elapsed_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
